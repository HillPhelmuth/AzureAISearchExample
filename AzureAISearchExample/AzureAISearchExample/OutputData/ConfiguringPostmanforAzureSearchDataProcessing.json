{
  "Id": "318",
  "Title": "\u0022Configuring Postman for Azure Search Data Processing\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdf\u0022metadata_storage_path\u0022,          \u0022mappingFunction\u0022 : { \u0022name\u0022 : \u0022base64Encode\u0022 }        },        {          \u0022sourceFieldName\u0022: \u0022metadata_storage_name\u0022,          \u0022targetFieldName\u0022: \u0022metadata_storage_name\u0022        }   ],  \u0022outputFieldMappings\u0022 :  [    {          \u0022sourceFieldName\u0022: \u0022/document/merged_text\u0022,          \u0022targetFieldName\u0022: \u0022content\u0022        },        {            \u0022sourceFieldName\u0022 :\r\n\u0022/document/normalized_images/*/text\u0022,            \u0022targetFieldName\u0022 : \u0022text\u0022        },      {          \u0022sourceFieldName\u0022 : \u0022/document/organizations\u0022,          \u0022targetFieldName\u0022 : \u0022organizations\u0022        },        {          \u0022sourceFieldName\u0022: \u0022/document/language\u0022,          \u0022targetFieldName\u0022: \u0022language\u0022        },      {          \u0022sourceFieldName\u0022 : \u0022/document/persons\u0022,          \u0022targetFieldName\u0022 :\r\n\u0022persons\u0022        },      {          \u0022sourceFieldName\u0022 : \u0022/document/locations\u0022,          \u0022targetFieldName\u0022 : \u0022locations\u0022        },        {          \u0022sourceFieldName\u0022 : \u0022/document/pages/*/keyPhrases/*\u0022,          \u0022targetFieldName\u0022 : \u0022keyPhrases\u0022        }    ],  \u0022parameters\u0022:  {  \u0022batchSize\u0022: 1,    \u0022maxFailedItems\u0022:-1,    \u0022maxFailedItemsPerBatch\u0022:-1,    \u0022configuration\u0022:3.\r\nSend the request. Postman should return a status code of 201 confirmingsuccessful processing.Expect this step to take several minutes to complete. Even though the data set issmall, analytical skills are computation-intensive.The script sets \u0022maxFailedItems\u0022 to -1, which instructs the indexing engine to ignoreerrors during data import. This is acceptable because there are so few documents in thedemo data source. For a larger data source, you would set the value to greater than 0.The \u0022dataToExtract\u0022:\u0022contentAndMetadata\u0022 statement tells the indexer to automaticallyextract the values from the blob\u0027s content property and the metadata of each object.\n"
}