{
  "Id": "1277",
  "Title": "\u0022Optimizing Query Performance with Lucene Syntax and Tokenization\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdfSearch Documents explains how to construct a query request, using either simplesyntax or full Lucene syntax for wildcard and regular expressions.For partial term queries, such as querying \u00223-6214\u0022 to find a match on \u0022\u002B1 (425)703-6214\u0022, you can use the simple syntax: search=3-6214\u0026queryType=simple.\r\nFor infix and suffix queries, such as querying \u0022num\u0022 or \u0022numeric to find a match on\u0022alphanumeric\u0022, use the full Lucene syntax and a regular expression:search=/.*num.*/\u0026queryType=fullIf you implement the recommended configuration that includes the keyword_v2tokenizer and lower-case token filter, you might notice a decrease in query performancedue to the extra token filter processing over existing tokens in your index.\r\nThe following example adds an EdgeNGramTokenFilter to make prefix matches faster.More tokens are generated for in 2-25 character combinations that include characters:(not only MS, MSF, MSFT, MSFT/, MSFT/S, MSFT/SQ, MSFT/SQL).As you can imagine, the extra tokenization results in a larger index.\r\nIf you have sufficientcapacity to accommodate the larger index, this approach with its faster response time4 - Build and testTune query performancemight be a better solution.JSONThis article explains how analyzers both contribute to query problems and solve queryproblems. As a next step, take a closer look at analyzer impact on indexing and queryprocessing. In particular, consider using the Analyze Text API to return tokenized outputso that you can see exactly what an analyzer is creating for your index.Tutorial: Create a custom analyzer for phone numbersLanguage analyzers{ \u0022fields\u0022: [   {   \u0022name\u0022: \u0022accountNumber\u0022,   \u0022analyzer\u0022:\u0022myCustomAnalyzer\u0022,   \u0022type\u0022: \u0022Edm.\n"
}