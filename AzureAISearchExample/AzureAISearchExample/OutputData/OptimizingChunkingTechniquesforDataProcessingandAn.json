{
  "Id": "708",
  "Title": "\u0022Optimizing Chunking Techniques for Data Processing and Analysis\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdfHere are some common chunking techniques, starting with the most widely usedmethod:Fixed-size chunks: Define a fixed size that\u0027s sufficient for semantically meaningfulparagraphs (for example, 200 words) and allows for some overlap (for example, 10-15% of the content) can produce good chunks as input for embedding vectorgenerators.Variable-sized chunks based on content: Partition your data based on contentcharacteristics, such as end-of-sentence punctuation marks, end-of-line markers,or using features in the Natural Language Processing (NLP) libraries. Markdownlanguage structure can also be used to split the data.\r\nCustomize or iterate over one of the above techniques. For example, when dealingwith large documents, you might use variable-sized chunks, but also append thedocument title to chunks from the middle of the document to prevent context loss.When you chunk data, overlapping a small amount of text between chunks can helppreserve context.\r\nWe recommend starting with an overlap of approximately 10%. Forexample, given a fixed chunk size of 256 tokens, you would begin testing with anoverlap of 25 tokens. The actual amount of overlap varies depending on the type ofdata and the specific use case, but we have found that 10-15% works for manyscenarios.\r\nWhen it comes to chunking data, think about these factors:Shape and density of your documents. If you need intact text or passages, largerchunks and variable chunking that preserves sentence structure can produce betterresults.User queries: Larger chunks and overlapping strategies help preserve context andsemantic richness for queries that target specific information. Common chunking techniquesContent overlap considerationsFactors for chunking dataLarge Language Models (LLM) have performance guidelines for chunk size. youneed to set a chunk size that works best for all of the models you\u0027re using. Forinstance, if you use models for summarization and embeddings, choose an optimalchunk size that works for both.\n"
}