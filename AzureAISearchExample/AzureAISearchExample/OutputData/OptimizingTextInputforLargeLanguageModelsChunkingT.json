{
  "Id": "707",
  "Title": "\u0022Optimizing Text Input for Large Language Models: Chunking Techniques\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdfChunking is only required if source documentsare too large for the maximum input size imposed by models.The models used to generate embedding vectors have maximum limits on the textfragments provided as input.\r\nFor example, the maximum length of input text for theAzure OpenAI embedding models is 8,191 tokens. Given that each token is around 4characters of text for common OpenAI models, this maximum limit is equivalent toaround 6000 words of text. If you\u0027re using these models to generate embeddings, it\u0027scritical that the input text stays under the limit.\r\nPartitioning your content into chunksensures that your data can be processed by the Large Language Models (LLM) used forindexing and queries.Because there isn\u0027t a native chunking capability in either Azure AI Search or AzureOpenAI, if you have large documents, you must insert a chunking step into indexing andquery workflows that breaks up large text.\r\nSome libraries that provide chunking include:LangChainSemantic KernelBoth libraries support common chunking techniques for fixed size, variable size, or acombination. You can also specify an overlap percentage that duplicates a small amount\uFF17 NoteThis article applies to the generally available version of vector search, whichassumes your application code calls an external library that performs datachunking.\r\nA new feature called integrated vectorization, currently in preview, offersembedded data chunking. Integrated vectorization takes a dependency onindexers, skillsets, and the Text Split skill.Why is chunking important?How chunking fits into the workflowof content in each chunk for context preservation. Here are some common chunking techniques, starting with the most widely usedmethod:Fixed-size chunks: Define a fixed size that\u0027s sufficient for semantically meaningfulparagraphs (for example, 200 words) and allows for some overlap (for example, 10-15% of the content) can produce good chunks as input for embedding vectorgenerators.Variable-sized chunks based on content: Partition your data based on contentcharacteristics, such as end-of-sentence punctuation marks, end-of-line markers,or using features in the Natural Language Processing (NLP) libraries. Markdownlanguage structure can also be used to split the data.\n"
}