{
  "Id": "513",
  "Title": "\u0022Optimizing Azure AI Search: Best Practices and Limitations\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdfAzure OpenAI token-per-minute limits are per model, per subscription. Keep this inmind if you\u0027re using an embedding model for both query and indexing workloads. Ifpossible, follow best practices. Have an embedding model for each workload, and try todeploy them in different subscriptions.On Azure AI Search, remember there are service limits by tier and workloads.\r\nFinally, the following features aren\u0027t currently supported:Customer-managed encryption keysShared private link connections to a vectorizerCurrently, there\u0027s no batching for integrated data chunking and vectorization\uEA80 TipTry the new Import and vectorize data wizard in the Azure portal to exploreintegrated vectorization before writing any code.\r\nOr, configure a Jupyter notebook to run the same workflow, cell by cell, to see howeach step works.LimitationsHere are some of the key benefits of the integrated vectorization:No separate data chunking and vectorization pipeline. Code is simpler to write andmaintain.Automate indexing end-to-end. When data changes in the source (such as inAzure Storage, Azure SQL, or Cosmos DB), the indexer can move those updatesthrough the entire pipeline, from retrieval, to document cracking, through optionalAI-enrichment, data chunking, vectorization, and indexing.Projecting chunked content to secondary indexes.\r\nSecondary indexes are createdas you would any search index (a schema with fields and other constructs), butthey\u0027re populated in tandem with a primary index by an indexer. Content fromeach source document flows to fields in primary and secondary indexes during thesame indexing run.Secondary indexes are intended for data chunking and Retrieval AugmentedGeneration (RAG) apps. Assuming a large PDF as a source document, the primaryindex might have basic information (title, date, author, description), and asecondary index has the chunks of content. Vectorization at the data chunk levelmakes it easier to find relevant information (each chunk is searchable) and return arelevant response, especially in a chat-style search app.\n"
}