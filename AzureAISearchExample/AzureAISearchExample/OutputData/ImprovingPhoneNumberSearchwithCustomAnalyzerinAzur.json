{
  "Id": "354",
  "Title": "\u0022Improving Phone Number Search with Custom Analyzer in Azure\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdfThinkingback to the search results from the previous step, we can start to see why those resultswere returned.In the first query, the incorrect phone numbers were returned because one of theirterms, 555, matched one of the terms we searched. In the second query, only the onenumber was returned because it was the only record that had a term matching4255550100.\r\nNow that we understand the results we\u0027re seeing, let\u0027s build a custom analyzer toimprove the tokenization logic.The goal is to provide intuitive search against phone numbers no matter what formatthe query or indexed string is in. To achieve this result, we\u0027ll specify a character filter, atokenizer, and a token filter.\r\nCharacter filters are used to process text before it\u0027s fed into the tokenizer. Common usesof character filters include filtering out HTML elements or replacing special characters.For phone numbers, we want to remove whitespace and special characters because notall phone number formats contain the same special characters and spaces.\r\nJSON  \u0022analyzer\u0022: \u0022standard.lucene\u0022}{    \u0022tokens\u0022: [        {            \u0022token\u0022: \u00224255550100\u0022,            \u0022startOffset\u0022: 0,            \u0022endOffset\u0022: 10,            \u0022position\u0022: 0        }    ]}5 - Build a custom analyzerCharacter filtersThe filter above will remove - ( ) \u002B . and spaces from the input.\r\nInputOutput(321) 555-01993215550199321.555.01993215550199Tokenizers split text into tokens and discard some characters, such as punctuation, alongthe way. In many cases, the goal of tokenization is to split a sentence into individualwords.For this scenario, we\u0027ll use a keyword tokenizer, keyword_v2, because we want to capturethe phone number as a single term. Note that this isn\u0027t the only way to solve thisproblem. See the Alternate approaches section below.Keyword tokenizers always output the same text it was given as a single term.InputOutputThe dog swims.[The dog swims.]3215550199[3215550199]\u0022charFilters\u0022: [    {      \u0022@odata.type\u0022: \u0022#Microsoft.Azure.Search.\n"
}