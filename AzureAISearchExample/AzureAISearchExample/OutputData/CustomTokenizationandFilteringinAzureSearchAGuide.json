{
  "Id": "769",
  "Title": "\u0022Custom Tokenization and Filtering in Azure Search: A Guide\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdf1tokenizer_nametokenizer_type Description andOptionswhitespace(type applies only when options areavailable)Divides text atwhitespace. Tokensthat are longer than255 characters aresplit. Tokenizer Types are always prefixed in code with #Microsoft.Azure.Search such thatClassicTokenizer would actually be specified as#Microsoft.Azure.Search.ClassicTokenizer.\r\nWe removed the prefix to reduce the widthof the table, but please remember to include it in your code. Notice that tokenizer_typeis only provided for tokenizers that can be customized. If there are no options, as is thecase with the letter tokenizer, there\u0027s no associated #Microsoft.Azure.Search type.A token filter is used to filter out or modify the tokens generated by a tokenizer.\r\nForexample, you can specify a lowercase filter that converts all characters to lowercase. Youcan have multiple token filters in a custom analyzer. Token filters run in the order inwhich they\u0027re listed.In the table below, the token filters that are implemented using Apache Lucene arelinked to the Lucene API documentation.\r\ntoken_filter_nametoken_filter_type Description and Optionsarabic_normalization(type applies only when options areavailable)A token filter that appliesthe Arabic normalizer tonormalize the orthography.apostrophe(type applies only when options areavailable)Strips all characters after anapostrophe (including theapostrophe itself).\r\nasciifoldingAsciiFoldingTokenFilterConverts alphabetic,numeric, and symbolicUnicode characters whicharen\u0027t in the first 127 ASCIIcharacters (the Basic LatinUnicode block) into theirASCII equivalents, if oneexists.11Token filters\uFF89Expand table1token_filter_nametoken_filter_type Description and OptionsOptionspreserveOriginal (type: bool)- If true, the original token iskept. The default is false.cjk_bigramCjkBigramTokenFilterForms bigrams of CJK termsthat are generated fromStandardTokenizer.OptionsignoreScripts (type: stringarray) - Scripts to ignore.Allowed values include: han,hiragana, katakana, hangul.The default is an empty list.\n"
}