{
  "Id": "234",
  "Title": "\u0022Optimizing Batch Sizes for Efficient Azure Search Indexing in C#\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdfWriteLine(\u0022{0} \\t\\t {1} \\t\\t {2} \\t\\t {3} \\t {4}\u0022, numDocs, Math.Round(sizeInMb, 3), Math.Round(sizeInMb / numDocs, 3), Math.Round(avgDuration, 3), Math.Round(mbPerSecond, 3));        // Pausing 2 seconds to let the search service catch its breathBecause not all documents are the same size (although they are in this sample), weestimate the size of the data we\u0027re sending to the search service.\r\nWe do this using thefunction below that first converts the object to json and then determines its size inbytes. This technique allows us to determine which batch sizes are most efficient interms of MB/s indexing speeds.C#The function requires an SearchClient as well as the number of tries you\u0027d like to testfor each batch size. As there may be some variability in indexing times for each batch,we try each batch three times by default to make the results more statisticallysignificant.C#When you run the function, you should see an output like below in your console:        Thread.\r\nSleep(2000);    }    Console.WriteLine();}// Returns size of object in MBpublic static double EstimateObjectSize(object data){    // converting object to byte[] to determine the size of the data    BinaryFormatter bf = new BinaryFormatter();    MemoryStream ms = new MemoryStream();    byte[] Array;    // converting data to json for more accurate sizing    var json = JsonSerializer. Serialize(data);    bf.Serialize(ms, json);    Array = ms.ToArray();    // converting from bytes to megabytes    double sizeInMb = (double)Array.Length / 1000000;    return sizeInMb;}await TestBatchSizesAsync(searchClient, numTries: 3);Identify which batch size is most efficient and then use that batch size in the next step ofthe tutorial.\n"
}