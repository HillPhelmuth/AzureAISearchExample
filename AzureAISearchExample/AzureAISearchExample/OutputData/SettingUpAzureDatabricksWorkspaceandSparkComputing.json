{
  "Id": "241",
  "Title": "\u0022Setting Up Azure Databricks Workspace and Spark Computing Platform\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdfIn this tutorial, Azure Databricks provides the Spark computing platform and theinstructions in the link will tell you how to set up the workspace. For this tutorial, weused the portal steps in \u0022Create a workspace\u0022.In this section, you\u0027ll create a cluster, install the synapseml library, and create a notebookto run the code.\r\n1. In Azure portal, find your Azure Databricks workspace and select Launchworkspace.2. On the left menu, select Compute.3. Select Create cluster.4. Give the cluster a name, accept the default configuration, and then create thecluster. It takes several minutes to create the cluster.5.\r\nInstall the synapseml library after the cluster is created:2341234\uFF17 NoteAll of the above Azure resources support security features in the Microsoft Identityplatform. For simplicity, this tutorial assumes key-based authentication, usingendpoints and keys copied from the portal pages of each service. If you implementthis workflow in a production environment, or share the solution with others,remember to replace hard-coded keys with integrated security or encrypted keys.1 - Create a Spark cluster and notebooka. Select Library from the tabs at the top of the cluster\u0027s page.b. Select Install new.c. Select Maven.d. In Coordinates, enter com.\r\nmicrosoft.azure:synapseml_2.12:0.10.0e. Select Install.6. On the left menu, select Create \u003E Notebook.7. Give the notebook a name, select Python as the default language, and select thecluster that has the synapseml library.8. Create seven consecutive cells. You\u0027ll paste code into each one.Paste the following code into the first cell of your notebook. Replace the placeholderswith endpoints and access keys for each resource. No other modifications are required,so run the code when you\u0027re ready.This code imports multiple packages and sets up access to the Azure resources used inthis workflow.Python2 - Set up dependenciesimport osfrom pyspark.sql.\n"
}