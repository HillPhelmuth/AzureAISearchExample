{
  "Id": "399",
  "Title": "\u0022Optimizing Vector Search with Embeddings and Nearest Neighbor Algorithms\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdfDocuments about cats would be close together,but farther from the dogs cluster while still being in the neighborhood for animals.Dissimilar concepts such as cloud computing would be much farther away. In practice,these embedding spaces are abstract and don\u0027t have well-defined, human-interpretablemeanings, but the core idea stays the same.\r\nEmbeddings and vectorizationWhat is the embedding space?In vector search, the search engine searches through the vectors within the embeddingspace to identify those that are near to the query vector. This technique is called nearestneighbor search. Nearest neighbors help quantify the similarity between items. A highdegree of vector similarity indicates that the original data was similar too.\r\nTo facilitatefast nearest neighbor search, the search engine will perform optimizations or employdata structures or data partitioning to reduce the search space. Each vector searchalgorithm will have different approaches to this problem, trading off differentcharacteristics such as latency, throughput, recall, and memory. To compute similarity,similarity metrics provide the mechanism for computing this distance.Azure AI Search currently supports the following algorithms:Hierarchical Navigable Small World (HNSW): HNSW is a leading ANN algorithmoptimized for high-recall, low-latency applications where data distribution isunknown or can change frequently. It organizes high-dimensional data points intoa hierarchical graph structure that enables fast and scalable similarity search whileallowing a tunable a trade-off between search accuracy and computational cost.Because the algorithm requires all data points to reside in memory for fast randomaccess, this algorithm consumes vector index size quota.Exhaustive K-nearest neighbors (KNN): Calculates the distances between the queryvector and all data points. It\u0027s computationally intensive, so it works best forsmaller datasets. Because the algorithm doesn\u0027t require fast random access of datapoints, this algorithm doesn\u0027t consume vector index size quota.\n"
}