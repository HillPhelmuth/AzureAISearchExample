{
  "Id": "398",
  "Title": "\u0022Effective Embeddings for Vector Search in Azure AI Search\u0022",
  "Text": "C:\\Users\\adamh\\Downloads\\azure-search.pdfDuring training, they learn to represent any input as a vector of realnumbers in an intermediary step called the encoder. After training is complete, theselanguage models can be modified so the intermediary vector representation becomesthe model\u0027s output. The resulting embeddings are high-dimensional vectors, wherewords with similar meanings are closer together in the vector space, as explained inUnderstand embeddings (Azure OpenAI).\r\nThe effectiveness of vector search in retrieving relevant information depends on theeffectiveness of the embedding model in distilling the meaning of documents andqueries into the resulting vector. The best models are well-trained on the types of datathey\u0027re representing.\r\nYou can evaluate existing models such as Azure OpenAI text-embedding-ada-002, bring your own model that\u0027s trained directly on the problemspace, or fine-tune a general-purpose model. Azure AI Search doesn\u0027t imposeconstraints on which model you choose, so pick the best one for your data.In order to create effective embeddings for vector search, it\u0027s important to take inputsize limitations into account. We recommend following the guidelines for chunking databefore generating embeddings. This best practice ensures that the embeddingsaccurately capture the relevant information and enable more efficient vector search.Embedding space is the corpus for vector queries.\r\nWithin a search index, it\u0027s all of thevector fields populated with embeddings from the same embedding model. Machinelearning models create the embedding space by mapping individual words, phrases, ordocuments (for natural language processing), images, or other forms of data into arepresentation comprised of a vector of real numbers representing a coordinate in ahigh-dimensional space. In this embedding space, similar items are located closetogether, and dissimilar items are located farther apart.For example, documents that talk about different species of dogs would be clusteredclose together in the embedding space. Documents about cats would be close together,but farther from the dogs cluster while still being in the neighborhood for animals.Dissimilar concepts such as cloud computing would be much farther away. In practice,these embedding spaces are abstract and don\u0027t have well-defined, human-interpretablemeanings, but the core idea stays the same.\n"
}